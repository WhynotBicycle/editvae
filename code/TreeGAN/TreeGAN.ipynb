{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import os.path as osp\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/shidi/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'coverage_from_generator' from 'measurement' (/home/shidi/3d-generate/code/3D-VAN-GAN/measurement.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-591e0b22e75e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/shidi/3d-generate/code/3D-VAN-GAN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoverage_from_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquality_from_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoverage_from_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'coverage_from_generator' from 'measurement' (/home/shidi/3d-generate/code/3D-VAN-GAN/measurement.py)"
     ]
    }
   ],
   "source": [
    "from model.gan_network import Generator, Discriminator\n",
    "from model.gradient_penalty import GradientPenalty\n",
    "\n",
    "import sys\n",
    "sys.path.append('code/utils')\n",
    "from measurement import coverage_from_generator_test\n",
    "# from measurement import quality_from_generator\n",
    "# from measurement import coverage_from_batch\n",
    "from loss_function import ChamferDistance\n",
    "from loss_function import ChamferLoss\n",
    "\n",
    "from encoder_decoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "workers = 6\n",
    "point_num = 2048\n",
    "G_FEAT=[96, 256, 256, 256, 128, 128, 128, 3]\n",
    "D_FEAT=[3,  64,  128, 256, 512, 1024]\n",
    "DEGREE=[1,  2,   2,   2,   2,   2,   64]\n",
    "support=10\n",
    "lambdaGP=10\n",
    "g_lr=1e-4\n",
    "d_lr=1e-4\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Data loader ####################################\n",
    "print('Loading data.........')\n",
    "# Root directory for dataset\n",
    "dataroot = '/home/shidi/3d-generate/data/shapeNet/ShapeNetCore.v2/'\n",
    "\n",
    "category = 'Airplane'\n",
    "path = osp.join(osp.dirname(osp.abspath('')), '..', 'data', 'shapeNet', 'ShapeNetCore.v2')\n",
    "# when in .py file\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'shapeNet', 'ShapeNetCore.v2')\n",
    "\n",
    "transform = T.Compose([\n",
    "#     T.RandomTranslate(0.01), #translate node positions by randomly sampled translation value, within (-0.01, 0.01)\n",
    "#     T.RandomRotate(15, axis = 0), # rotate axis 0 with degree sampled in (-15, 15)\n",
    "#     T.RandomRotate(15, axis = 1),\n",
    "#     T.RandomRotate(15, axis=2),\n",
    "    T.FixedPoints(point_num)\n",
    "])\n",
    "pre_transform = T.NormalizeScale() # centers and normalizes node positions to the interval (-1, 1)\n",
    "train_dataset = ShapeNet(path, category, split='trainval', transform=transform,\n",
    "                        pre_transform=pre_transform)\n",
    "test_dataset = ShapeNet(path, category, split='test',\n",
    "                        pre_transform=pre_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeGAN():\n",
    "    def __init__(self, train_loader, test_loader):\n",
    "        # ------------------------------------------------Dataset---------------------------------------------- #\n",
    "        self.test_loader = test_loader\n",
    "        self.train_loader = train_loader\n",
    "        # ----------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "        # -------------------------------------------------Module---------------------------------------------- #\n",
    "        self.G = Generator(batch_size=batch_size, features=G_FEAT, degrees=DEGREE, support=support).to(device)\n",
    "        self.D = Discriminator(batch_size=batch_size, features=D_FEAT).to(device)\n",
    "        self.E = Encoder(dim=G_FEAT[0]).to(device)\n",
    "        \n",
    "        self.optimizerG = optim.Adam(self.G.parameters(), lr=g_lr, betas=(0, 0.99))\n",
    "        self.optimizerD = optim.Adam(self.D.parameters(), lr=d_lr, betas=(0, 0.99))\n",
    "        self.optimizerE = optim.Adam(self.E.parameters(), lr=g_lr, betas=(0, 0.99))\n",
    "\n",
    "        self.GP = GradientPenalty(lambdaGP, gamma=1, device=device)\n",
    "        self.EncoderLoss = ChamferLoss()\n",
    "        print(\"Network prepared.\")\n",
    "        # ----------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        self.c_losses = []\n",
    "#         self.save_path = '../../data/TreeGAN/baseline/'\n",
    "        self.save_path = 'test/'\n",
    "        \n",
    "        self.train_len = len(train_loader)\n",
    "        self.whole_batchs = len(test_loader)+len(train_loader)\n",
    "    \n",
    "    def _save_losses(self, filename, data):\n",
    "        with open(self.save_path+'losses/'+filename,'w') as f:\n",
    "            for line in data:\n",
    "                f.write(str(line) + '\\n')\n",
    "        \n",
    "    def save_losses(self):\n",
    "        self._save_losses('d_losses.txt', self.d_losses)\n",
    "        self._save_losses('g_losses.txt', self.g_losses)\n",
    "        self._save_losses('c_losses.txt', self.c_losses)\n",
    "    \n",
    "    def save_model(self):\n",
    "        torch.save(self.G.state_dict(), self.save_path+'model/g'+category+str(epochs)+'.pth')\n",
    "        torch.save(self.D.state_dict(), self.save_path+'model/d'+category+str(epochs)+'.pth')\n",
    "    \n",
    "    def load(self):\n",
    "        self.G.load_state_dict(torch.load(self.save_path+'model/g'+category+str(epochs)+'.pth'))\n",
    "        self.D.load_state_dict(torch.load(self.save_path+'model/d'+category+str(epochs)+'.pth'))\n",
    "    \n",
    "    def show_examples(self):\n",
    "        z = torch.randn(batch_size, 1, 96).to(device)\n",
    "        tree = [z]\n",
    "        fake_point = self.G(tree)\n",
    "        for i in range(batch_size):\n",
    "            out = fake_point[i].unsqueeze(0)\n",
    "            out_color = torch.as_tensor(torch.tensor([0, 0, 255]).repeat(out.size()[1], 1), dtype=torch.int).unsqueeze(0)\n",
    "            writer.add_mesh('generated samples '+str(i), vertices=out, colors=out_color)\n",
    "            \n",
    "    def save_generated(self):\n",
    "        for i in range(392):\n",
    "            z = torch.randn(batch_size, 1, 96).to(device)\n",
    "            tree = [z]\n",
    "            fake_point = self.G(tree)\n",
    "            torch.save(fake_point, self.save_path+'generated/'+str(i)+'.pt')\n",
    "        print('All saved !')\n",
    "    \n",
    "    def coverage(self):\n",
    "        return coverage_from_generator_test(self.test_loader, self.save_path+'generated/', device)\n",
    "#         return quality_from_generator(self.train_loader, self.save_path+'generated/', device)\n",
    "#     def coverage_recon(self):\n",
    "#         return coverage_from_batch(self.test_loader, self.G, device, True, self.E)\n",
    "\n",
    "    def run(self, save_ckpt=None, load_ckpt=None, result_path=None):  \n",
    "        writer = SummaryWriter('../../runs/WGAN_'+category+str(epochs))\n",
    "        for epoch in range(0, epochs):\n",
    "            for _iter, data in enumerate(self.train_loader):\n",
    "                point = pad_sequence([inp['pos'] for inp in data.to_data_list()], batch_first=True)\n",
    "                point = point.to(device)\n",
    "#                 print(point.size())\n",
    "                # -------------------- Discriminator -------------------- #\n",
    "                for d_iter in range(5):\n",
    "                    self.D.zero_grad()\n",
    "                    \n",
    "                    z = torch.randn(batch_size, 1, 96).to(device)\n",
    "                    tree = [z]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        fake_point = self.G(tree)         \n",
    "                        \n",
    "                    D_real = self.D(point)\n",
    "                    D_realm = D_real.mean()\n",
    "\n",
    "                    D_fake = self.D(fake_point)\n",
    "                    D_fakem = D_fake.mean()\n",
    "\n",
    "                    gp_loss = self.GP(self.D, point.data, fake_point.data)\n",
    "                    \n",
    "                    d_loss = -D_realm + D_fakem\n",
    "                    d_loss_gp = d_loss + gp_loss\n",
    "                    d_loss_gp.backward()\n",
    "                    self.optimizerD.step()\n",
    "                # ---------------------- Generator ---------------------- #\n",
    "                self.G.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size, 1, 96).to(device)\n",
    "                tree = [z]\n",
    "                \n",
    "                fake_point = self.G(tree)\n",
    "                G_fake = self.D(fake_point)\n",
    "                G_fakem = G_fake.mean()\n",
    "                \n",
    "                g_loss = -G_fakem\n",
    "                g_loss.backward()\n",
    "                self.optimizerG.step()\n",
    "                \n",
    "                gmcd = ChamferDistance(point, fake_point)\n",
    "                # --------------------- Encoder & Generator -------------- #\n",
    "                z, _ = self.E(point.transpose(2,1))\n",
    "                z = z.unsqueeze(1)\n",
    "                if z.size(0) == batch_size: ## because code for TreeGAN only deal with fixed number of batch size\n",
    "                    self.E.zero_grad()\n",
    "                    self.G.zero_grad()\n",
    "                    tree = [z]\n",
    "                    fake_point = self.G(tree)\n",
    "                    mcd = self.EncoderLoss(point, fake_point)\n",
    "                    mcd.backward()\n",
    "                    self.optimizerE.step()\n",
    "                    self.optimizerG.step()\n",
    "                else: # only for ploting\n",
    "                    mcd = ChamferDistance(point, fake_point)\n",
    "                # --------------------- Chamfer distance ----------------- #\n",
    "                self.d_losses.append(d_loss.item())\n",
    "                self.g_losses.append(g_loss.item())\n",
    "                self.c_losses.append(mcd.item())\n",
    "                # --------------------- Visualization -------------------- #\n",
    "#                 print(_iter)\n",
    "#                 if _iter % 30 == 29:\n",
    "                print(\"[Epoch/Iter] \", \"{:3} / {:3}\".format(epoch, _iter),\n",
    "                      \"[ D_Loss ] \", \"{: 7.6f}\".format(d_loss), \n",
    "                      \"[ G_Loss ] \", \"{: 7.6f}\".format(g_loss),\n",
    "                      \"[ Chamfer Loss] \", \"{: 7.6f}\".format(mcd),\n",
    "                      \"[ Generated Chamfer Loss] \", \"{: 7.6f}\".format(gmcd))\n",
    "\n",
    "            # ---------------------- Plot on the tensorboard ------------- #\n",
    "            ## print the first point cloud in the last batch each epoch\n",
    "            if epoch % 50 == 0:\n",
    "    #             ipt = point[0].unsqueeze(0)\n",
    "                out = fake_point[0].unsqueeze(0)\n",
    "                out_color = torch.as_tensor(torch.tensor([0, 0, 255]).repeat(out.size()[1], 1), dtype=torch.int).unsqueeze(0)\n",
    "                writer.add_mesh('output '+ str(epoch), vertices=out, colors=out_color)\n",
    "                \n",
    "            # ---------------------- Save losses & model ----------------- #\n",
    "            self.save_losses()\n",
    "            self.save_model()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TreeGAN(train_loader, test_loader)\n",
    "# print(model.G)\n",
    "model.run()\n",
    "# model.load()\n",
    "# model.save_generated()\n",
    "# model.coverage()\n",
    "# model.coverage_recon()\n",
    "# model.show_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
